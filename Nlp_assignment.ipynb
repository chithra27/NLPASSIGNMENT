{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAK9WZtRjyxl",
        "outputId": "0bf8aea1-3132-48b9-dff0-4fa759008bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Common Phrases:\n",
            "\n",
            "Top 2-grams:\n",
            "organization university: 216\n",
            "distribution world: 145\n",
            "writes article: 116\n",
            "space station: 101\n",
            "henry spencer: 96\n",
            "world nntppostinghost: 89\n",
            "_ _: 83\n",
            "prbaccessdigexcom pat: 81\n",
            "space shuttle: 76\n",
            "dont know: 74\n",
            "\n",
            "Top 3-grams:\n",
            "distribution world nntppostinghost: 89\n",
            "u toronto zoology: 67\n",
            "henryzootorontoedu henry spencer: 55\n",
            "xnewsreader tin version: 51\n",
            "tin version 11: 51\n",
            "_ _ _: 48\n",
            "organization express access: 43\n",
            "express access online: 43\n",
            "access online communications: 43\n",
            "baalkekelvinjplnasagov ron baalke: 38\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text by removing punctuation, converting to lowercase, tokenizing, and removing stop words.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of filtered tokens.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation and lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    return filtered_tokens\n",
        "\n",
        "def extract_ngrams(tokens, n):\n",
        "    \"\"\"\n",
        "    Extract n-grams from a list of tokens.\n",
        "\n",
        "    Parameters:\n",
        "    tokens (list): A list of tokens.\n",
        "    n (int): The number of words in each n-gram.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of n-grams.\n",
        "    \"\"\"\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "def identify_common_phrases(text, ngram_range=(2, 3), top_n=10):\n",
        "    \"\"\"\n",
        "    Identify common phrases in the input text by extracting n-grams and analyzing their frequencies.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text.\n",
        "    ngram_range (tuple): The range of n-grams to extract (min_n, max_n).\n",
        "    top_n (int): The number of top n-grams to return.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary with n-grams and their frequencies.\n",
        "    \"\"\"\n",
        "    filtered_tokens = preprocess_text(text)\n",
        "    common_phrases = {}\n",
        "\n",
        "    for n in range(ngram_range[0], ngram_range[1] + 1):\n",
        "        ngrams = extract_ngrams(filtered_tokens, n)\n",
        "        ngram_freq = Counter(ngrams)\n",
        "        common_phrases[n] = ngram_freq.most_common(top_n)\n",
        "\n",
        "    return common_phrases\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_data = fetch_20newsgroups(subset='train', categories=['sci.space', 'comp.graphics'])\n",
        "dataset_text = ' '.join(newsgroups_data.data)  # Combine all text into a single string\n",
        "\n",
        "# Identify common phrases in the dataset\n",
        "common_phrases = identify_common_phrases(dataset_text)\n",
        "\n",
        "# Display common phrases\n",
        "print(\"\\nCommon Phrases:\")\n",
        "for n, phrases in common_phrases.items():\n",
        "    print(f\"\\nTop {n}-grams:\")\n",
        "    for phrase, freq in phrases:\n",
        "        print(f'{phrase}: {freq}')"
      ]
    }
  ]
}